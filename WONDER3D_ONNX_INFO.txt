================================================================================
                        Wonder3D ONNX Model Specification
================================================================================

Model: Wonder3D (xxlong0)
Repository: https://github.com/xxlong0/Wonder3D
License: Apache 2.0
Task: Single-Image to Multi-View 3D Generation with Cross-Domain Diffusion

================================================================================
                                  OVERVIEW
================================================================================

Wonder3D generates textured 3D meshes from single images in 2-3 minutes using
a cross-domain diffusion approach:

1. Multi-view generation: Creates consistent normal maps and color images
   across 6 viewpoints (0°, 45°, 90°, 180°, -90°, -45° azimuth at 0° elevation)

2. Mesh reconstruction: Fuses the generated views into a 3D model using
   Neural Signed Distance reconstruction (NeuS or instant-nsr-pl)

Key Features:
- Cross-domain attention for RGB + Normal consistency
- 6-view generation at 256x256 resolution
- SAM-based background removal
- NeuS mesh reconstruction

================================================================================
                               ONNX COMPONENTS
================================================================================

The export produces four ONNX files:

1. wonder3d_image_encoder.onnx
------------------------------
Purpose: CLIP image encoder for conditioning

INPUTS:
  Name: image
  Shape: (batch_size, 3, 224, 224)
  Type: float32
  Description: Input image for CLIP encoding
  Preprocessing:
    - Resize to 224x224
    - Normalize: (pixel / 255.0 - [0.48145466, 0.4578275, 0.40821073]) /
                                  [0.26862954, 0.26130258, 0.27577711]

OUTPUTS:
  Name: image_embeddings
  Shape: (batch_size, 257, 1280)
  Type: float32
  Description: CLIP ViT-H image embeddings (256 patches + CLS token)


2. wonder3d_vae_encoder.onnx
----------------------------
Purpose: Encode images to latent space

INPUTS:
  Name: image
  Shape: (batch_size, 3, 256, 256)
  Type: float32
  Description: RGB or Normal image in [-1, 1] range

OUTPUTS:
  Name: latent
  Shape: (batch_size, 4, 32, 32)
  Type: float32
  Description: VAE latent (scaled by 0.18215)


3. wonder3d_vae_decoder.onnx
----------------------------
Purpose: Decode latent to images

INPUTS:
  Name: latent
  Shape: (batch_size, 4, 32, 32)
  Type: float32
  Description: VAE latent representation

OUTPUTS:
  Name: image
  Shape: (batch_size, 3, 256, 256)
  Type: float32
  Description: Decoded image in [-1, 1] range


4. wonder3d_unet.onnx
---------------------
Purpose: Cross-domain diffusion UNet for denoising

INPUTS:
  Name: latent
  Shape: (batch_size, 48, 32, 32)  # 6 views * 2 domains * 4 channels
  Type: float32
  Description: Noisy latent for all views (RGB + Normal)

  Name: timestep
  Shape: (batch_size,)
  Type: int64
  Description: Diffusion timestep (0-1000)

  Name: encoder_hidden_states
  Shape: (batch_size, 77, 768)
  Type: float32
  Description: CLIP text/image conditioning

OUTPUTS:
  Name: noise_pred
  Shape: (batch_size, 48, 32, 32)
  Type: float32
  Description: Predicted noise to subtract

================================================================================
                            MODEL PARAMETERS
================================================================================

Architecture:
  - Base: Stable Diffusion 2.1
  - UNet: Modified for multi-view cross-attention
  - VAE: SD 2.1 VAE (KL-autoencoder)
  - Image Encoder: CLIP ViT-H/14

View Configuration:
  - Number of views: 6
  - Azimuth angles: [0°, 45°, 90°, 180°, -90°, -45°]
  - Elevation: 0° (horizontal ring)
  - Output resolution: 256x256 per view

Diffusion Settings:
  - Scheduler: DDIM
  - Steps: 50 (default)
  - CFG Scale: 3.0 (classifier-free guidance)

Memory Requirements:
  - VAE: ~165 MB
  - UNet: ~3.4 GB
  - CLIP: ~1.7 GB
  - Total: ~5.5 GB VRAM for inference

================================================================================
                              C# USAGE EXAMPLE
================================================================================

using Microsoft.ML.OnnxRuntime;
using Microsoft.ML.OnnxRuntime.Tensors;

public class Wonder3DInference : IDisposable
{
    private InferenceSession _imageEncoderSession;
    private InferenceSession _vaeEncoderSession;
    private InferenceSession _vaeDecoderSession;
    private InferenceSession _unetSession;

    private const int NUM_VIEWS = 6;
    private const int LATENT_CHANNELS = 4;
    private const int LATENT_SIZE = 32;
    private const int IMAGE_SIZE = 256;
    private const int CLIP_SIZE = 224;
    private const int NUM_STEPS = 50;
    private const float CFG_SCALE = 3.0f;

    // CLIP normalization
    private static readonly float[] CLIP_MEAN = { 0.48145466f, 0.4578275f, 0.40821073f };
    private static readonly float[] CLIP_STD = { 0.26862954f, 0.26130258f, 0.27577711f };

    public Wonder3DInference(string modelDir)
    {
        var options = new SessionOptions();
        try { options.AppendExecutionProvider_CUDA(0); } catch { }

        _imageEncoderSession = new InferenceSession(
            Path.Combine(modelDir, "wonder3d_image_encoder.onnx"), options);
        _vaeEncoderSession = new InferenceSession(
            Path.Combine(modelDir, "wonder3d_vae_encoder.onnx"), options);
        _vaeDecoderSession = new InferenceSession(
            Path.Combine(modelDir, "wonder3d_vae_decoder.onnx"), options);
        _unetSession = new InferenceSession(
            Path.Combine(modelDir, "wonder3d_unet.onnx"), options);
    }

    /// <summary>
    /// Generate 6-view RGB and Normal images from single input.
    /// </summary>
    public (float[][,,,] rgbViews, float[][,,,] normalViews) GenerateViews(
        byte[] inputImage)
    {
        // 1. Encode input image with CLIP
        var clipInput = PreprocessForCLIP(inputImage);
        var imageEmbeddings = EncodeImage(clipInput);

        // 2. Initialize noise for 6 views * 2 domains
        var latent = InitializeNoise(NUM_VIEWS * 2);

        // 3. DDIM denoising loop
        var scheduler = new DDIMScheduler(NUM_STEPS);

        for (int step = 0; step < NUM_STEPS; step++)
        {
            int timestep = scheduler.GetTimestep(step);

            // Predict noise with classifier-free guidance
            var noisePred = PredictNoise(latent, timestep, imageEmbeddings);

            // Apply CFG
            var unconditionalPred = PredictNoise(latent, timestep, GetNullEmbedding());
            for (int i = 0; i < noisePred.Length; i++)
            {
                noisePred[i] = unconditionalPred[i] + CFG_SCALE *
                              (noisePred[i] - unconditionalPred[i]);
            }

            // DDIM step
            latent = scheduler.Step(noisePred, timestep, latent);
        }

        // 4. Decode all views
        var rgbViews = new float[NUM_VIEWS][,,,];
        var normalViews = new float[NUM_VIEWS][,,,];

        for (int v = 0; v < NUM_VIEWS; v++)
        {
            // Extract latent for this view
            var rgbLatent = ExtractViewLatent(latent, v * 2);
            var normalLatent = ExtractViewLatent(latent, v * 2 + 1);

            rgbViews[v] = DecodeLatent(rgbLatent);
            normalViews[v] = DecodeLatent(normalLatent);
        }

        return (rgbViews, normalViews);
    }

    private DenseTensor<float> PreprocessForCLIP(byte[] imageData)
    {
        var tensor = new DenseTensor<float>(new[] { 1, 3, CLIP_SIZE, CLIP_SIZE });

        // Resize and normalize for CLIP
        for (int y = 0; y < CLIP_SIZE; y++)
        {
            for (int x = 0; x < CLIP_SIZE; x++)
            {
                int srcIdx = (y * CLIP_SIZE + x) * 4;

                for (int c = 0; c < 3; c++)
                {
                    float val = imageData[srcIdx + c] / 255.0f;
                    tensor[0, c, y, x] = (val - CLIP_MEAN[c]) / CLIP_STD[c];
                }
            }
        }

        return tensor;
    }

    private float[] EncodeImage(DenseTensor<float> image)
    {
        var inputs = new List<NamedOnnxValue>
        {
            NamedOnnxValue.CreateFromTensor("image", image)
        };

        using var results = _imageEncoderSession.Run(inputs);
        return results.First().AsTensor<float>().ToArray();
    }

    private float[] InitializeNoise(int numLatents)
    {
        var random = new Random(42);
        var noise = new float[numLatents * LATENT_CHANNELS * LATENT_SIZE * LATENT_SIZE];

        for (int i = 0; i < noise.Length; i++)
        {
            // Standard normal distribution
            double u1 = 1.0 - random.NextDouble();
            double u2 = 1.0 - random.NextDouble();
            noise[i] = (float)(Math.Sqrt(-2.0 * Math.Log(u1)) *
                              Math.Sin(2.0 * Math.PI * u2));
        }

        return noise;
    }

    private float[] PredictNoise(float[] latent, int timestep, float[] conditioning)
    {
        var inputs = new List<NamedOnnxValue>
        {
            NamedOnnxValue.CreateFromTensor("latent",
                new DenseTensor<float>(latent,
                    new[] { 1, NUM_VIEWS * 2 * LATENT_CHANNELS, LATENT_SIZE, LATENT_SIZE })),
            NamedOnnxValue.CreateFromTensor("timestep",
                new DenseTensor<long>(new[] { (long)timestep }, new[] { 1 })),
            NamedOnnxValue.CreateFromTensor("encoder_hidden_states",
                new DenseTensor<float>(conditioning, new[] { 1, 77, 768 }))
        };

        using var results = _unetSession.Run(inputs);
        return results.First().AsTensor<float>().ToArray();
    }

    private float[,,,] DecodeLatent(float[] viewLatent)
    {
        var inputs = new List<NamedOnnxValue>
        {
            NamedOnnxValue.CreateFromTensor("latent",
                new DenseTensor<float>(viewLatent,
                    new[] { 1, LATENT_CHANNELS, LATENT_SIZE, LATENT_SIZE }))
        };

        using var results = _vaeDecoderSession.Run(inputs);
        var output = results.First().AsTensor<float>();

        // Convert to array format
        var image = new float[1, 3, IMAGE_SIZE, IMAGE_SIZE];
        // Copy data...

        return image;
    }

    private float[] ExtractViewLatent(float[] allLatents, int viewIndex)
    {
        int viewSize = LATENT_CHANNELS * LATENT_SIZE * LATENT_SIZE;
        var viewLatent = new float[viewSize];
        Array.Copy(allLatents, viewIndex * viewSize, viewLatent, 0, viewSize);
        return viewLatent;
    }

    private float[] GetNullEmbedding()
    {
        // Return empty/null conditioning for CFG
        return new float[77 * 768];
    }

    public void Dispose()
    {
        _imageEncoderSession?.Dispose();
        _vaeEncoderSession?.Dispose();
        _vaeDecoderSession?.Dispose();
        _unetSession?.Dispose();
    }
}

================================================================================
                          DDIM SCHEDULER FOR C#
================================================================================

public class DDIMScheduler
{
    private int[] _timesteps;
    private float[] _alphas;
    private float[] _alphasCumprod;

    public DDIMScheduler(int numSteps, int numTrainTimesteps = 1000)
    {
        // Create timestep schedule
        _timesteps = new int[numSteps];
        float stepRatio = (float)numTrainTimesteps / numSteps;

        for (int i = 0; i < numSteps; i++)
        {
            _timesteps[i] = (int)(numTrainTimesteps - 1 - i * stepRatio);
        }

        // Compute alphas (cosine schedule)
        _alphasCumprod = new float[numTrainTimesteps];
        for (int i = 0; i < numTrainTimesteps; i++)
        {
            float t = (float)i / numTrainTimesteps;
            _alphasCumprod[i] = (float)Math.Pow(Math.Cos((t + 0.008) /
                                1.008 * Math.PI / 2), 2);
        }
    }

    public int GetTimestep(int step) => _timesteps[step];

    public float[] Step(float[] noisePred, int timestep, float[] sample)
    {
        int prevTimestep = timestep > 0 ?
            _timesteps[Array.IndexOf(_timesteps, timestep) + 1] : 0;

        float alphaProdT = _alphasCumprod[timestep];
        float alphaProdTPrev = prevTimestep >= 0 ?
            _alphasCumprod[prevTimestep] : 1.0f;

        float betaProdT = 1 - alphaProdT;
        float betaProdTPrev = 1 - alphaProdTPrev;

        float sqrtAlphaProdT = (float)Math.Sqrt(alphaProdT);
        float sqrtBetaProdT = (float)Math.Sqrt(betaProdT);
        float sqrtAlphaProdTPrev = (float)Math.Sqrt(alphaProdTPrev);

        // Predicted x0
        var predX0 = new float[sample.Length];
        for (int i = 0; i < sample.Length; i++)
        {
            predX0[i] = (sample[i] - sqrtBetaProdT * noisePred[i]) / sqrtAlphaProdT;
        }

        // Direction pointing to x_t
        float sqrtBetaProdTPrev = (float)Math.Sqrt(betaProdTPrev);

        var result = new float[sample.Length];
        for (int i = 0; i < sample.Length; i++)
        {
            result[i] = sqrtAlphaProdTPrev * predX0[i] +
                       sqrtBetaProdTPrev * noisePred[i];
        }

        return result;
    }
}

================================================================================
                        MESH RECONSTRUCTION NOTES
================================================================================

After generating 6-view RGB and Normal images, reconstruct the mesh:

Option 1: Neural Signed Distance (NeuS)
- Train a small MLP to predict SDF from multi-view input
- Run marching cubes on SDF grid
- Best quality, but slower

Option 2: Multi-View Stereo (MVS)
- Use classical stereo matching on generated views
- Faster, but may have holes

Option 3: Direct Depth Estimation
- Estimate depth from normal maps
- Fuse into point cloud
- Run Poisson reconstruction

Camera Matrices for 6 Views (orthographic):
- View 0 (0°):   [1, 0, 0]
- View 1 (45°):  [0.707, 0, 0.707]
- View 2 (90°):  [0, 0, 1]
- View 3 (180°): [-1, 0, 0]
- View 4 (-90°): [0, 0, -1]
- View 5 (-45°): [0.707, 0, -0.707]

================================================================================
                             PREPROCESSING NOTES
================================================================================

Input Image Requirements:
  - Object centered, occupying ~80% of image height
  - Clean or removed background (use SAM)
  - Good lighting, minimal shadows

Background Removal:
  - Use Segment Anything (SAM) for best results
  - Replace background with white
  - Ensure clean object edges

Image Normalization:
  - For CLIP: ImageNet normalization
  - For VAE: Scale to [-1, 1]

================================================================================
                              ERROR HANDLING
================================================================================

1. "Inconsistent views generated"
   - Increase CFG scale (try 4.0-5.0)
   - Add more diffusion steps (75-100)
   - Check CLIP conditioning quality

2. "Artifacts in normal maps"
   - Use higher denoising strength
   - Check input image quality
   - Verify background is removed

3. "Mesh reconstruction fails"
   - Ensure all 6 views are consistent
   - Check normal map directions
   - Verify camera calibration

================================================================================
                              VERSION INFO
================================================================================

Export Script Version: 1.0
ONNX Opset: 14
Base Model: Stable Diffusion 2.1
Tested ONNX Runtime: 1.17.1

================================================================================
